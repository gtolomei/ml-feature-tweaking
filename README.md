# Tweaking Features of Ensembles of Machine-Learned Trees
This repository contains the source code associated with the method proposed by Tolomei _et al._ in their KDD 2017 research paper entitled "_Interpretable Predictions of Tree-based Ensembles via Actionable Feature Tweaking_" \[more information available at: [KDD 2017 website](http://www.kdd.org/kdd2017/papers/view/interpretable-predictions-of-tree-based-ensembles-via-actionable-feature-tw) or [arXiv.org](https://arxiv.org/abs/1706.06691)\]<br />
Please, cite this work using the following <a href="http://dblp.uni-trier.de/rec/bibtex/conf/kdd/TolomeiSHL17" target="_blank">BibTeX entry</a>.

**NOTE:** _This work has been developed by the authors of the paper while working at Yahoo Labs, London, UK. Although the method proposed is general and applicable to several different domains, the authors validate it on an online advertising use case. In particular, they demonstrate the ability of this approach to generate **actionable recommendations** for improving the quality of the ads served by Yahoo Gemini.<br /> 
Due to confidentiality, any business-related detail has been removed from this repository, which however can still be used by other researchers working on related topics, such as ML model interpretability or adversarial ML just to name a few._

This repo is made up of **3** scripts which are supposed to be run in the same order as follows:
1.  <code>**dump_paths.py**</code>
2.  <code>**tweak_features.py**</code>
3.  <code>**compute_tweaking_costs.py**</code>

## 1. <code>**dump_paths.py**</code>
The first stage of the pipeline is accomplished by this script, which can be invoked as follows:

```bash
> ./dump_paths.py ${PATH_TO_SERIALIZED_MODEL} ${PATH_TO_OUTPUT_FILE}
```

where:<br />
<code>**${PATH_TO_SERIALIZED_MODEL}**</code> is the path to the (binary) file containing a serialized, trained binary classifier (i.e., a <code>**scikit-learn**</code> tree-based ensemble estimator).<br />
<code>**${PATH_TO_OUTPUT_FILE}**</code> is the path where the output file will be stored. This file will contain a plain-text representation of all the _positive paths_, namely all the paths extracted from all the trees in the ensemble whose leaves are labeled as _positive_.<br />
Each line of the output file is a positive path, which in turn is a sequence of boolean tests with the following format:

<code>**[tree_id, [(feature_id, op, value), ..., (feature_id, op, value)]**</code>

where:<br />
-  <code>**tree_id**</code> is the unique id of the tree within the ensemble.<br />
-  <code>**feature_id**</code> is the unique id of the feature subject of the test.<br />
-  <code>**op**</code> is the operator of the test: either <code>**'<='**</code> or <code>**'>'**</code>.<br />
-  <code>**value**</code> is the value against which the feature is tested.

## 2. <code>**tweak_features.py**</code>
The second stage of the pipeline is actually the _core_ of the entire process. The script can be run as follows:

```bash
> ./tweak_features.py ${PATH_TO_DATASET} ${PATH_TO_SERIALIZED_MODEL} ${PATH_TO_POSITIVE_PATHS_FILE} \
${PATH_TO_OUTPUT_FILE} [--epsilon=x]
```

where:<br />
<code>**${PATH_TO_DATASET}**</code> is the path to the dataset file used to train the binary classifier. This is assumed to be either a <code>**.tsv**</code> or a <code>**.csv**</code> file, where each line is an instance and each field is a feature. The very last field is supposed to be the target label (named <code>**'class'**</code>).<br />
<code>**${PATH_TO_SERIALIZED_MODEL}**</code> as above.<br />
<code>**${PATH_TO_POSITIVE_PATHS_FILE}**</code> is the path to the output file generated by the previous script <code>**dump_paths.py**</code> at stage 1.<br />
<code>**${PATH_TO_OUTPUT_DIRECTORY}**</code> is the path to the directory where the output file will be stored. This file will be called <code>**transformations_${EPSILON}.tsv**</code>, where <code>**${EPSILON}**</code> is the value of <code>**epsilon**</code> optional argument (by default <code>**epsilon=0.1**</code>).

## 3. <code>**compute_tweaking_costs.py**</code>
Once the set of _candidate feature transformations_ (i.e., tweakings) have been successfully calculated, we can measure the actual costs of those transformations. This can be achieved by running the following script:

```bash
> ./compute_tweaking_costs.py  ${PATH_TO_DATASET} \
${PATH_TO_TRANSFORMATIONS} \
${PATH_TO_OUTPUT_DIRECTORY} \
--costfuncs=unmatched_component_rate,euclidean_distance,cosine_distance,jaccard_distance,pearson_correlation_distance
```

where:<br />
<code>**${PATH_TO_DATASET}**</code> is the path to the dataset file used to train the binary classifier, as above.<br />
<code>**${PATH_TO_TRANSFORMATIONS}**</code> is the path to the file containing the candidate transformations obtained with step 2.<br />
<code>**${PATH_TO_OUTPUT_DIRECTORY}**</code> is the path to the directory where the output file will be stored.
Finally, the optional argument <code>**costfuncs**</code> will contain a list of functions used to compute the cost of each transformation (default <code>**costfuncs=euclidean_distance**</code>.

The ultimate result of this step is the creation of 2 tsv files inside <code>**${PATH_TO_OUTPUT_DIRECTORY}**</code> containing the _costs_ and the _signs_ of each transformation.

Additional steps can be performed using those files as input, depending on the final task goal.
